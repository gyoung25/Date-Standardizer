{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6858a86b",
   "metadata": {},
   "source": [
    "This code is based on an assignment from the DeepLearning.AI's Coursera course titled Sequence Models. I have expanded on the assignment by writing two models with different architectures and writing a helper function that facilitates model evaluation.\n",
    "\n",
    "**Skills demonstrated:** LSTMs, attention, RNNs, NLP, tensorflow, data processing, model evaluation, general coding\n",
    "\n",
    "Overview: \n",
    "\n",
    "> Long-Short Term Memory (LSTM) networks are powerful tools for natural language processing (NLP) tasks. They expand on basic recurrent neural network (RNN) models by encoding a \"memory\" that addresses the vanishing gradient problem that persists in classic RNNs. Attention models expand on traditional RNNs by passing more information between encoder and decoder layers, thereby allowing the model to learn important features connecting parts of text that appear more than a few words apart. This code contains three LSTM models: one without attention and a more traditional architecture, one with attention, and one with a naive approximation to attention that simply uses fully connected layers to pass information between the encoder and decoder layers.\n",
    "\n",
    "Models:\n",
    "\n",
    "> 1. LSTM without attention. Encoder layer is a bidirectional LSTM. Output from the last timestep is fed into a (unidirectional) LSTM decoder layer.\n",
    "2. LSTM with attention. Encoder layer is a bidirectional LSTM. Output <i> from each time step </i> **and** the hidden state from the previous time step of the decoder layer is fed into an attention layer, which passes the LSTM output and hidden state through a dense layer, then through a softmax layer that is used to calculate the context. The context is then fed into the decoder layer, which is a (unidirectional) LSTM.\n",
    "3. LSTM with approximated attention. Similar to the LSTM model with attention, but the attention layer does not receive the hidden state from the decoder layer and passes the output of the dense layer directly to the decoder layer.\n",
    "\n",
    "Purpose: \n",
    "> Demonstrate the effectiveness of LSTMs with attention performing NLP tasks.\n",
    "\n",
    "Datasets: \n",
    "\n",
    "> A randomly generated dataset containing a prescribed number of dates in a variety of human-readable formats\n",
    "\n",
    "Target:\n",
    "\n",
    "> The given date in machine-readable format. (Assumptions: If no day is specified in the human-readable date, assign the day as the first of the month. If no month is specified, assign January.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9068cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "from tensorflow.keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from tensorflow.keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "#from babel.dates import format_date\n",
    "from date_std_tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66aa1d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 20000/20000 [00:03<00:00, 6034.71it/s]\n"
     ]
    }
   ],
   "source": [
    "#Generate data\n",
    "m = 20000\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e361159",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset[:50]\n",
    "#print('Number of characters in human_vocab: ' + str(len(human_vocab)))\n",
    "#print('Number of characters in machine_vocab: ' + str(len(machine_vocab)))\n",
    "#print('Characters in human_vocab: ' + str([x[0] for x in list(human_vocab.items())]))\n",
    "#print('Characters in machine_vocab: ' + str([x[0] for x in list(machine_vocab.items())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91e00252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (20000, 20)\n",
      "Y.shape: (20000, 10)\n",
      "Xoh.shape: (20000, 20, 37)\n",
      "Yoh.shape: (20000, 10, 11)\n"
     ]
    }
   ],
   "source": [
    "#Define the max length of human-readable dates (Tx) and machine readable dates (Ty)\n",
    "#In the context of the model, Tx is the max character length of an input string and Ty is the length of the output\n",
    "Tx = 20\n",
    "Ty = 10\n",
    "\n",
    "#X (Y): lists of length Tx representing each human-readable (machine-readable) date. Dimension (m,Tx) ((m,Ty))\n",
    "#Xoh (Yoh): one-hot representation of X (Y). Dimension (m,Tx,len(human_vocab)) ((m,Ty,len(machine_vocab)))\n",
    "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"Y.shape:\", Y.shape)\n",
    "print(\"Xoh.shape:\", Xoh.shape)\n",
    "print(\"Yoh.shape:\", Yoh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7be82431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define LSTM layer sizes\n",
    "n_a = 32 # number of units for the bi-directional LSTM's hidden state a\n",
    "n_s = 64 # number of units for the uni-directional LSTM's hidden state s\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74df97bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define shared layers as global variables for model_with_attention\n",
    "repeat = RepeatVector(Tx)\n",
    "concat = Concatenate(axis=-1)\n",
    "dense1 = Dense(10, activation = \"relu\")\n",
    "dense2 = Dense(1, activation = \"relu\")\n",
    "activate = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "dot = Dot(axes = 1)\n",
    "\n",
    "post_attention_LSTM = LSTM(n_s, return_state = True)\n",
    "softmax_layer_attn = Dense(len(machine_vocab), activation=softmax)\n",
    "\n",
    "model_attn = model_with_attention(Tx, Ty, n_a, n_s,\n",
    "                                  len(human_vocab), len(machine_vocab),\n",
    "                                  repeat, concat, dense1, dense2, activate, dot,\n",
    "                                  post_attention_LSTM, softmax_layer_attn)\n",
    "opt = Adam(learning_rate=0.005, weight_decay=0.005) # Adam(...) \n",
    "model_attn.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])\n",
    "\n",
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "outputs = list(Yoh.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0a5f399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 19s 60ms/step - loss: 0.2753 - dense_2_loss: 0.0169 - dense_2_1_loss: 0.0170 - dense_2_2_loss: 0.0678 - dense_2_3_loss: 0.1474 - dense_2_4_loss: 1.8870e-04 - dense_2_5_loss: 0.0012 - dense_2_6_loss: 0.0023 - dense_2_7_loss: 3.0350e-04 - dense_2_8_loss: 0.0093 - dense_2_9_loss: 0.0129 - dense_2_accuracy: 0.9912 - dense_2_1_accuracy: 0.9911 - dense_2_2_accuracy: 0.9707 - dense_2_3_accuracy: 0.9524 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9999 - dense_2_6_accuracy: 1.0000 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.9979 - dense_2_9_accuracy: 0.9970\n"
     ]
    }
   ],
   "source": [
    "start_attn = time.time()\n",
    "model_attn.fit([Xoh, s0, c0], outputs, epochs=num_epochs, batch_size=64)\n",
    "end_attn = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c130b320",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_attn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bd46a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define shared LSTM layer for model_approx_attention\n",
    "LSTM_approx = LSTM(units=n_s, return_state = True)\n",
    "softmax_layer_approx = Dense(len(machine_vocab), activation=softmax)\n",
    "\n",
    "model_approx_attn = model_approx_attention(Tx, Ty, n_a, n_s,\n",
    "                                           len(human_vocab), len(machine_vocab),\n",
    "                                           LSTM_approx, softmax_layer_approx)\n",
    "\n",
    "opt = Adam(learning_rate=0.005, weight_decay=0.005) # Adam(...) \n",
    "model_approx_attn.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])\n",
    "\n",
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "outputs = list(Yoh.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "debce9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 43s 136ms/step - loss: 0.5171 - dense_3_loss: 0.0191 - dense_3_1_loss: 0.0189 - dense_3_2_loss: 0.0822 - dense_3_3_loss: 0.1922 - dense_3_4_loss: 1.3641e-04 - dense_3_5_loss: 0.0153 - dense_3_6_loss: 0.0825 - dense_3_7_loss: 2.7081e-04 - dense_3_8_loss: 0.0566 - dense_3_9_loss: 0.0500 - dense_3_accuracy: 0.9916 - dense_3_1_accuracy: 0.9909 - dense_3_2_accuracy: 0.9668 - dense_3_3_accuracy: 0.9391 - dense_3_4_accuracy: 1.0000 - dense_3_5_accuracy: 0.9947 - dense_3_6_accuracy: 0.9756 - dense_3_7_accuracy: 1.0000 - dense_3_8_accuracy: 0.9816 - dense_3_9_accuracy: 0.9865\n"
     ]
    }
   ],
   "source": [
    "start_approx = time.time()\n",
    "model_approx_attn.fit([Xoh, s0, c0], outputs, epochs=num_epochs, batch_size=64)\n",
    "end_approx = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfd32a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_approx_attn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c6bdde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define shared LSTM layer for model_just_LSTM\n",
    "LSTM_just = LSTM(units=n_s, return_state = True)\n",
    "softmax_layer_just = Dense(len(machine_vocab), activation=softmax)\n",
    "\n",
    "model_just_LSTM = model_just_LSTM(Tx, Ty, n_a, n_s,\n",
    "                                  len(human_vocab), len(machine_vocab),\n",
    "                                  LSTM_just, softmax_layer_just) \n",
    "\n",
    "opt = Adam(learning_rate=0.005, weight_decay=0.005) # Adam(...) \n",
    "model_just_LSTM.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])\n",
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "outputs = list(Yoh.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac9a2a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 8s 27ms/step - loss: 1.0958 - dense_14_loss: 0.0258 - dense_14_1_loss: 0.0237 - dense_14_2_loss: 0.1484 - dense_14_3_loss: 0.2743 - dense_14_4_loss: 4.0678e-04 - dense_14_5_loss: 0.0265 - dense_14_6_loss: 0.1001 - dense_14_7_loss: 0.0010 - dense_14_8_loss: 0.2398 - dense_14_9_loss: 0.2558 - dense_14_accuracy: 0.9900 - dense_14_1_accuracy: 0.9901 - dense_14_2_accuracy: 0.9505 - dense_14_3_accuracy: 0.9233 - dense_14_4_accuracy: 1.0000 - dense_14_5_accuracy: 0.9920 - dense_14_6_accuracy: 0.9765 - dense_14_7_accuracy: 0.9999 - dense_14_8_accuracy: 0.9091 - dense_14_9_accuracy: 0.9082\n"
     ]
    }
   ],
   "source": [
    "start_just = time.time()\n",
    "model_just_LSTM.fit([Xoh, s0, c0], outputs, epochs=num_epochs, batch_size=64)\n",
    "end_just = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d00948eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_just_LSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e690b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_weights(\"LSTM_model.weights.h5\")\n",
    "#Load pre-trained params if not fitting\n",
    "#model.load_weights(\"LSTM_model.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93a8baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load list of human-readable dates from dates.txt\n",
    "doc = []\n",
    "with open('dates.txt') as file:\n",
    "    for line in file:\n",
    "        doc.append(line)\n",
    "\n",
    "doc = [x.lower().strip().replace(',','').replace('.','') for x in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e2155fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 4s 9ms/step\n",
      "LSTM with attention:\n",
      "Time to train 3 epochs : 109.63 seconds.\n",
      "Accuracy: 0.996\n",
      "Number of dates incorrectly predicted: 2\n",
      "Incorrectly predicted dates in the form (Input date, predicted date, actual date):\n",
      "[('10/04/1998', '1998-10-01', '1998-10-04'), ('november 2004', '2004-11-00', '2004-11-01')]\n"
     ]
    }
   ],
   "source": [
    "acc_attn, wrong_attn, _ = model_performance_test(model_attn, doc, human_vocab, inv_machine_vocab, Tx, n_s)\n",
    "print('LSTM with attention:')\n",
    "print('Time to train ' + str(num_epochs) + ' epochs : ' + f\"{end_attn-start_attn:.2f}\" + ' seconds.')\n",
    "print('Accuracy: ' + str(acc_attn))\n",
    "print('Number of dates incorrectly predicted: ' + str(len(wrong_attn)))\n",
    "print('Incorrectly predicted dates in the form (Input date, predicted date, actual date):\\n' + str(wrong_attn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77f77d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 5s 23ms/step\n",
      "LSTM with approximated attention:\n",
      "Time to train 3 epochs: 156.54 seconds.\n",
      "Accuracy: 0.966\n",
      "Number of dates incorrectly predicted: 17\n",
      "Incorrectly predicted dates in the form (Input date, predicted date, actual date):\n",
      "[('5/21/1977', '1977-04-21', '1977-05-21'), ('6/24/1987', '1987-05-24', '1987-06-24'), ('12/01/1973', '1973-12-00', '1973-12-01'), ('01 oct 1979', '1979-10-30', '1979-10-01'), ('04 dec 1988', '1988-02-04', '1988-12-04'), ('december 23 1999', '1999-02-23', '1999-12-23'), ('dec 2009', '2009-02-01', '2009-12-01'), ('june 1999', '1999-01-01', '1999-06-01'), ('12/2005', '2005-02-01', '2005-12-01'), ('12/2008', '2008-02-01', '2008-12-01'), ('12/2012', '2012-02-01', '2012-12-01'), ('12/2014', '2014-02-01', '2014-12-01'), ('12/1994', '1994-02-01', '1994-12-01'), ('12/1986', '1986-02-01', '1986-12-01'), ('12/2007', '2007-02-01', '2007-12-01'), ('12/1989', '1989-02-01', '1989-12-01'), ('12/2004', '2004-02-01', '2004-12-01')]\n"
     ]
    }
   ],
   "source": [
    "acc_approx, wrong_approx, _ = model_performance_test(model_approx_attn, doc, human_vocab, inv_machine_vocab, Tx, n_s)\n",
    "print('LSTM with approximated attention:')\n",
    "print('Time to train ' + str(num_epochs) + ' epochs: ' + f\"{end_approx-start_approx:.2f}\" + ' seconds.')\n",
    "print('Accuracy: ' + str(acc_approx))\n",
    "print('Number of dates incorrectly predicted: ' + str(len(wrong_approx)))\n",
    "print('Incorrectly predicted dates in the form (Input date, predicted date, actual date):\\n' + str(wrong_approx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0aaff31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 4s 5ms/step\n",
      "LSTM model, no attention:\n",
      "Time to train 3epochs : 51.82 seconds.\n",
      "Accuracy: 0.764\n",
      "Number of dates incorrectly predicted: 118\n",
      "Sample incorrectly predicted dates in the form (Input date, predicted date, actual date):\n",
      "[('7/8/1971', '1981-07-08', '1971-07-08'), ('2/6/1996', '1996-02-26', '1996-02-06'), ('4/10/1971', '1971-04-11', '1971-04-10'), ('4/09/1975', '1975-04-04', '1975-04-09'), ('8/01/1998', '1998-08-11', '1998-08-01'), ('1/25/2011', '2011-11-25', '2011-01-25'), ('4/12/1982', '1982-04-11', '1982-04-12'), ('10/13/1976', '1976-10-33', '1976-10-13'), ('7/21/1998', '1998-07-11', '1998-07-21'), ('10/21/1979', '1979-10-11', '1979-10-21')]\n"
     ]
    }
   ],
   "source": [
    "acc_just, wrong_just, _ = model_performance_test(model_just_LSTM, doc, human_vocab, inv_machine_vocab, Tx, n_s)\n",
    "print('LSTM model, no attention:')\n",
    "print('Time to train ' + str(num_epochs) + 'epochs : ' + f\"{end_just-start_just:.2f}\" + ' seconds.')\n",
    "print('Accuracy: ' + str(acc_just))\n",
    "print('Number of dates incorrectly predicted: ' + str(len(wrong_just)))\n",
    "print('Sample incorrectly predicted dates in the form (Input date, predicted date, actual date):\\n' + str(wrong_just[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dc5487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "909769ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#acc, wrong = model_performance_test(model_attn, doc, human_vocab, Tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97028c34",
   "metadata": {},
   "source": [
    "Next we will use mispelled dates to test how robust the models are to spelling errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "12c4220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_misspell = ['decembre 3, 2000', 'merch 1986', 'juen, 17, 1999', '29 fbreuary 2024', 'novmebr 35 20100']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e3975ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "LSTM with attention:\n",
      "Predicted dates in the form (Input date, model_attn predicted date, model_approx_attn predicted, model_just_LSTMs predicted):\n",
      "\n",
      "('decembre 3, 2000', '2000-12-03', '2000-12-03', '2000-12-03')\n",
      "\n",
      "('merch 1986', '1986-03-01', '1986-03-01', '1986-03-01')\n",
      "\n",
      "('juen, 17, 1999', '1999-06-17', '1999-06-17', '1999-07-17')\n",
      "\n",
      "('29 fbreuary 2024', '2024-02-29', '2024-02-29', '2024-02-09')\n",
      "\n",
      "('novmebr 35 20100', '2010-11-05', '2000-11-33', '2000-11-35')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m_pred = len(doc_misspell)\n",
    "s00 = np.zeros((m_pred, n_s))\n",
    "c00 = np.zeros((m_pred, n_s))\n",
    "_, Xoh_pred = prepare_input(doc_misspell, human_vocab, Tx)\n",
    "    \n",
    "pred_attn = model_attn.predict([Xoh_pred, s00, c00])\n",
    "pred_approx = model_approx_attn.predict([Xoh_pred, s00, c00])\n",
    "pred_just = model_just_LSTM.predict([Xoh_pred, s00, c00])\n",
    "    \n",
    "#convert output of the model into a list of human-readable dates\n",
    "output_attn = prepare_output(pred_attn, inv_machine_vocab)\n",
    "output_approx = prepare_output(pred_approx, inv_machine_vocab)\n",
    "output_just = prepare_output(pred_just, inv_machine_vocab)\n",
    "\n",
    "\n",
    "#Create a list of 4-tuples of (human_readable, model_attn_predicted, \n",
    "# model_approx_attn_predicted, model_just_LSTMs_predicted)\n",
    "input_output = list(zip(doc_misspell, output_attn, output_approx, output_just))\n",
    "\n",
    "print('LSTM with attention:')\n",
    "print('Predicted dates in the form (Input date, model_attn predicted date, model_approx_attn predicted, model_just_LSTMs predicted):\\n')\n",
    "for x in input_output:\n",
    "    print(str(x)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f3ac9f",
   "metadata": {},
   "source": [
    "The model with attention was best able to standardize each of the above misspelled dates among the three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63e68da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
